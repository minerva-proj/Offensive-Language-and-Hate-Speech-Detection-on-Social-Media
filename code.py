# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QVU5HaUN6h7CteH1neGTzh9T-NkEn2wY

# Student ID: 2200846

## Install required libraries
"""

!pip install transformers tensorflow

"""## import all require libraries. """

import warnings
warnings.filterwarnings("ignore")
import numpy as np
import os
import pandas as pd
import joblib
import re
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig
from tensorflow.keras.utils import custom_object_scope
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix
from tensorflow.keras.models import Sequential, load_model, Model
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Flatten, Input
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import pad_sequences
import torch
import datetime
from sklearn import svm
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import tensorflow as tf
from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from keras.callbacks import EarlyStopping
import pickle
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from tabulate import tabulate
nltk.download('vader_lexicon')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

"""**Define my student id as a variable**"""

student_id = 2200846 # My Student id

"""## set `seed` for all libraries"""

# set same seeds for all libraries
print('Setting seed for all librraies...')
np.random.seed(student_id)
seed= student_id
print('done!')

"""# Common Codes

## **GDrive access**
"""

# Mount Google Drive
print('mounting GDrive...')
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

"""## **Set global variables**

*   Set data and model path
*   Splitting the original dataset into 4 subsets - [25%, 50%, 75%, 100%]
"""

print('All the directories and global variables are defined-->')

# Defining the Paths
GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807/Assignment2/', str(student_id))
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('Google Drive path: ', GOOGLE_DRIVE_PATH)

# Check the contents of the Google Drive directory
print('Contents of Google Drive directory:')
print(os.listdir(GOOGLE_DRIVE_PATH))

# Load the training dataset from a CSV file
train_100_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')
df = pd.read_csv(train_100_file)


# Recursively divide the data into four splits
def recursive_split(df, seed):
    """
    Recursively splits a dataset into sizes of [25%, 50%, 75%, and 100%]
    while maintaining the class distribution of the original dataset.

    Args:
        df (pd.DataFrame): The input dataset.
        test_size (float): The size of the test set in each split. Default: 0.25.

    Returns:
        A dictionary containing the 4 splits of the dataset.
    """
    # Separate the DataFrame into positive and negative classes
    pos_df = df[df['label'] == 'OFF']
    neg_df = df[df['label'] == 'NOT']

    # Shuffle the positive and negative classes separately
    pos = pos_df.sample(frac=1, random_state=seed)
    neg = neg_df.sample(frac=1, random_state=seed)

    # Define the sizes of the subsets
    subset_sizes = [0.25, 0.5, 0.75, 1.0]

    # Initialize lists to store the resulting subsets
    subsets = [[] for _ in subset_sizes]

    # Loop through the subsets
    for i, subset_size in enumerate(subset_sizes):
        # Calculate the number of positive and negative examples in the subset
        pos_subset_size = int(np.ceil(subset_size * len(pos)))
        neg_subset_size = int(np.ceil(subset_size * len(neg)))

        # Extract the positive and negative examples for the subset
        pos_subset = pos_df[:pos_subset_size]
        neg_subset = neg_df[:neg_subset_size]

        # Concatenate the positive and negative subsets
        subset = pd.concat([pos_subset, neg_subset])

        # Shuffle the subset
        subset = subset.sample(frac=1, random_state=seed)

        # Store the resulting subset in the corresponding list
        subsets[i] = subset
    train_25, train_50, train_75, train_100 = subsets

    return train_25, train_50, train_75, train_100

# function usage
# Assign the resulting subsets to separate variables
train_25, train_50, train_75, train_100 = recursive_split(df, seed)

# Verify that the class distributions are maintained
# Calculate and print the class distribution for each split
print('Original dataset class distribution:')
print(df['label'].value_counts(normalize=True))
print('Total Count : ', len(df.index))
print()

print('Train Dataset Statistics of Different Size :')
table1 = [['25%', len(train_25.index), (train_25['label'].value_counts(normalize=True)['OFF'])*100, (train_25['label'].value_counts(normalize=True)['NOT'])*100],
          ['50%', len(train_50.index), (train_50['label'].value_counts(normalize=True)['OFF'])*100, (train_50['label'].value_counts(normalize=True)['NOT'])*100],
          ['75%', len(train_75.index), (train_75['label'].value_counts(normalize=True)['OFF'])*100, (train_75['label'].value_counts(normalize=True)['NOT'])*100],
          ['100%', len(df.index), (df['label'].value_counts(normalize=True)['OFF'])*100, (df['label'].value_counts(normalize=True)['NOT'])*100]]
print(tabulate(table1, headers=['Data %', 'Total', '% OFF', '% NOT'], tablefmt='orgtbl'))
print('\n')

print('Saving the files=============>')
# Save the divided subsets to CSV files
train_25_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv')
train_25.to_csv(train_25_file, index=False)
print('Saved 25% of the training data to :', train_25_file)

train_50_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_50.csv')
train_50.to_csv(train_50_file, index=False)
print('Saved 50% of the training data to :', train_50_file)

train_75_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_75.csv')
train_75.to_csv(train_75_file, index=False)
print('Saved 75% of the training data to :', train_75_file)

print('100% of the training data :', train_100_file)

print('\n')
# Load the validation and test datasets from CSV files
val_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')
print('Validation data :', val_file)
print(pd.read_csv(val_file)['label'].value_counts(normalize=True))
print('Total Count : ', len(pd.read_csv(val_file).index))
print()
print('\n')
test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')
print('Test data :', test_file)
print(pd.read_csv(test_file)['label'].value_counts(normalize=True))
print('Total Count : ', len(pd.read_csv(test_file).index))
print()

print('\n')
print('='*50)
print('=====================Model 1=====================')
print('\n')

MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1') # Model 1 directory
print('Model 1 directory: ', MODEL_1_DIRECTORY)

MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') # Model 1 trained using 25% of train data directory
print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)

model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data 
print('Output file name using model 1 using 25% of train data: ',model_1_25_output_test_file)

MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50') # Model 1 trained using 50% of train data directory
print('Model 1 directory with 50% data: ', MODEL_1_50_DIRECTORY)

model_1_50_output_test_file = os.path.join(MODEL_1_50_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 50% of train data 
print('Output file name using model 1 using 50% of train data: ',model_1_50_output_test_file)

MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75') # Model 1 trained using 75% of train data directory
print('Model 1 directory with 75% data: ', MODEL_1_75_DIRECTORY)

model_1_75_output_test_file = os.path.join(MODEL_1_75_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 75% of train data 
print('Output file name using model 1 using 75% of train data: ',model_1_75_output_test_file)

MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') # Model 1 trained using 100% of train data directory
print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)

model_1_100_output_test_file = os.path.join(MODEL_1_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 100% of train data 
print('Output file name using model 1 using 100% of train data: ',model_1_100_output_test_file)

print('\n')
print('='*50)
print('=====================Model 2=====================')
print('\n')

MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2') # Model 2 directory
print('Model 2 directory: ', MODEL_2_DIRECTORY)

MODEL_2_25_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25') # Model 2 trained using 25% of train data directory
print('Model 2 directory with 25% data: ', MODEL_2_25_DIRECTORY)

model_2_25_output_test_file = os.path.join(MODEL_2_25_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 25% of train data 
print('Output file name using model 2 using 25% of train data: ',model_2_25_output_test_file)

MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50') # Model 2 trained using 50% of train data directory
print('Model 2 directory with 50% data: ', MODEL_2_50_DIRECTORY)

model_2_50_output_test_file = os.path.join(MODEL_2_50_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 50% of train data 
print('Output file name using model 2 using 50% of train data: ',model_2_50_output_test_file)

MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75') # Model 2 trained using 75% of train data directory
print('Model 2 directory with 75% data: ', MODEL_2_75_DIRECTORY)

model_2_75_output_test_file = os.path.join(MODEL_2_75_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 75% of train data 
print('Output file name using model 2 using 75% of train data: ',model_2_75_output_test_file)

MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100') # Model 2 trained using 100% of train data directory
print('Model 2 directory with 100% data: ', MODEL_2_100_DIRECTORY)

model_2_100_output_test_file = os.path.join(MODEL_2_100_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 100% of train data 
print('Output file name using model 2 using 100% of train data: ',model_2_100_output_test_file)

print('\n')
print('='*50)
print('=====================Model 3=====================')
print('\n')

MODEL_3_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '3') # Model 3 directory
print('Model 3 directory: ', MODEL_3_DIRECTORY)

MODEL_3_25_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'25') # Model 3 trained using 25% of train data directory
print('Model 3 directory with 25% data: ', MODEL_3_25_DIRECTORY)

model_3_25_output_test_file = os.path.join(MODEL_3_25_DIRECTORY, 'output_test.csv') # Output file using Model 3 trained using 25% of train data 
print('Output file name using model 3 using 25% of train data: ',model_3_25_output_test_file)

MODEL_3_50_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'50') # Model 3 trained using 50% of train data directory
print('Model 3 directory with 50% data: ', MODEL_3_50_DIRECTORY)

model_3_50_output_test_file = os.path.join(MODEL_3_50_DIRECTORY, 'output_test.csv') # Output file using Model 3 trained using 50% of train data 
print('Output file name using model 3 using 50% of train data: ',model_3_50_output_test_file)

MODEL_3_75_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'75') # Model 3 trained using 75% of train data directory
print('Model 3 directory with 75% data: ', MODEL_3_75_DIRECTORY)

model_3_75_output_test_file = os.path.join(MODEL_3_75_DIRECTORY, 'output_test.csv') # Output file using Model 3 trained using 75% of train data 
print('Output file name using model 3 using 75% of train data: ',model_3_75_output_test_file)

MODEL_3_100_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'100') # Model 3 trained using 100% of train data directory
print('Model 3 directory with 100% data: ', MODEL_3_100_DIRECTORY)

model_3_100_output_test_file = os.path.join(MODEL_3_100_DIRECTORY, 'output_test.csv') # Output file using Model 3 trained using 100% of train data 
print('Output file name using model 3 using 100% of train data: ',model_3_100_output_test_file)

"""## **Exploring the dataset file**"""

print('EDA of the training dataset :')
print(df.head())
print('\n')

# Print the shape of the dataset
print('Shape of the dataset :')
print(df.shape)
print('\n')

# Check for missing values
print('Check missing values :')
print(df.isna().sum())
print('\n')

# Print descriptive statistics for the dataset
print('descriptive statistics for the dataset :')
print(df.describe())

# Plot the distribution of classes
sns.countplot(x='label', data=df)
plt.title('Class distribution')
plt.show()
print('\n')

print('Label Count Distribution :')
print('No. of tweets Offensive (OFF) are', len(df[df.label=='OFF']))
print('No. of tweets Not Offensive (NOT) are', len(df[df.label=='NOT']))

"""## Functions"""

def compute_performance(y_true, y_pred):
    """
    Computes performance metrics and displays them in a table.

    Args:
        y_true: numpy array or list - true labels
        y_pred: numpy array or list - predicted labels

    Returns:
        float - F1 score
    """
    # Calculate performance metrics
    acc = accuracy_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred, average='macro')
    pre = precision_score(y_true, y_pred, average='macro')
    f1 = f1_score(y_true, y_pred, average='macro')

    # Create table
    table = [['Accuracy', round(acc, 4)],
             ['Recall', round(rec, 4)],
             ['Precision', round(pre, 4)],
             ['F1 Score', round(f1, 4)]]

    # Print table
    print(tabulate(table, headers=['Metric', 'Value'], tablefmt='orgtbl'))

    # Plot confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Return F1 score
    return f1

def clean_text(data):
    """
    Preprocesses the training, testing, and validation data by nltk

    Args:
        data (str): The path to the data file.

    Returns:
        tuple: A tuple containing the preprocessed training, testing, and validation data, as well
            as the vectorizer.
    """
    # Load the data from the files
    # Tokenize the text data using NLTK
    data['tweet'] = data['tweet'].apply(lambda x: word_tokenize(x))

    # Remove stop words using NLTK
    stop_words = set(stopwords.words('english'))
    data['tweet'] = data['tweet'].apply(lambda x: [word for word in x if word.lower() not in stop_words])

    # Combine the tokenized text data back into strings
    data['tweet'] = data['tweet'].apply(lambda x: ' '.join(x))
    return data

def vectorizing(train_file):
    """
    Vectorizing the text data in the training file using TfidfVectorizer. Save the vectorizer

    Args:
        train_file (str): The path to the training data file.

    Returns:
        tuple: A tuple containing the preprocessed training and as well as the vectorizer.
    """

    # Vectorize the text data using TfidfVectorizer
    vectorizer = TfidfVectorizer()
    X_train = vectorizer.fit_transform(train_file["tweet"])
    y_train = train_file["label"]
    return X_train, y_train, vectorizer

"""# Method 1 Start

**This method, preprocesse the data with NLTK and TFIDF vectorizer and trains a Support Vector Machine (SVM) model.**

## Training Method 1 Code
"""

def train_method1(train_file, val_file, model_dir):
    """
    Trains a Support Vector Machine model using the preprocessed training data and saves the
    trained model to disk. Prints computed evaluation matrics

    Args:
        train_file (str): The path to the preprocessed training data file.
        val_file (str): The path to the preprocessed validation data file.
        model_dir (str): The path to the directory where the trained model will be saved.
    """

    print('========================TRAINING METHOD 1========================')
    # Load the dataset
    train_data = pd.read_csv(train_file)
    val_data = pd.read_csv(val_file)

    # Clean the data
    train_data = clean_text(train_data)
    val_data = clean_text(val_data)

    # Vectorize the data
    X_train, y_train, vectorizer = vectorizing(train_data)
    vec_path = os.path.join(model_dir, "vec.sav")
    joblib.dump(vectorizer, vec_path)

    X_val = vectorizer.transform(val_data['tweet'])
    y_val = val_data['label']



    # Train a Support Vector Machine model
    model = svm.SVC()
    model.fit(X_train, y_train)

    # Save the trained model to disk
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    model_path = os.path.join(model_dir, "model.sav")
    joblib.dump(model, model_path)

    # Predict on the validation set
    y_pred_val = model.predict(X_val)

      
    print('\n')
    print('Evaluation------------->')
    compute_performance(y_val, y_pred_val)


    print(f"Model saved to {model_path}")

    return

"""## Testing Method 1 Code"""

def test_method1(test_file, model_file, output_dir):
    """
    Loads a trained model from disk and evaluates it on the test data. Saves the
    classification report to disk.

    Args:
        test_file (str): The path to the preprocessed test data file.
        model_file (str): The path to the trained model file.
        output_dir (str): The path to the directory where the classification report will be saved.
    """
    print('========================TESTING METHOD 1========================')

    # Load test data
    test_data = pd.read_csv(test_file)
    
    # Load the trained model and vectorizer
    model = joblib.load(os.path.join(model_file, "model.sav"))
    vectorizer = joblib.load(os.path.join(model_file, "vec.sav"))

    # Preprocess the test data
    test_data = clean_text(test_data)

    # Vectorize the data
    X_test = vectorizer.transform(test_data['tweet'])
    y_test = test_data['label']

    # Make predictions on the test data
    y_pred_test = model.predict(X_test)

    print('\n')
    print('Evaluation------------->')
    # Compute Evaluation Matrics
    compute_performance(y_test, y_pred_test)


    # Save predictions to output file
    test_data['out_label'] = y_pred_test
    test_data.to_csv(output_dir, index=False)
    print(test_data)
    print("Testing completed successfully!")
    return

"""**Implementing the functions**"""

training_data = [
    (train_25_file, MODEL_1_25_DIRECTORY, model_1_25_output_test_file, '25%'),
    (train_50_file, MODEL_1_50_DIRECTORY, model_1_50_output_test_file, '50%'),
    (train_75_file, MODEL_1_75_DIRECTORY, model_1_75_output_test_file, '75%'),
    (train_100_file, MODEL_1_100_DIRECTORY, model_1_100_output_test_file, '100%')
]

for data in training_data:
    train_file, model_dir, output_file, data_label = data
    print(f'=====Training on {data_label} of the dataset=====')
    train_method1(train_file, val_file, model_dir)
    print('\n')
    print('='*100)
    print('\n')

"""**The best model was when trained and evaluated on 25% training dataset**"""

# testing on entire dataset
test_method1(test_file, MODEL_1_25_DIRECTORY, model_1_25_output_test_file)

"""## Method 1 End

# Method 2 Start

**This method preprocess the data with NLTK and TensorFlow's tokenizer and trains a BiLSTM model**

## Training Method 2 Code
"""

def train_method2(train_file, val_file, model_dir):
    """
    The function loads the training and validation data, cleans the data using NLTK, tokenizes it TensorFlow’s tokenizer, 
    and pads the sequences to ensure they are all the same length. 
    It also converts the labels to one-hot vectors.
    Trains a Bidirectional LSTM model using the training data and saves the
    trained model to disk.

    Args:
        train_file (str): The path to the preprocessed training data file.
        val_file (str): The path to the preprocessed validation data file.
        model_dir (str): The path to the directory where the trained model will be saved.
    """

    print('========================TRAINING METHOD 2========================')
    # Load the dataset
    train_data = pd.read_csv(train_file)
    val_data = pd.read_csv(val_file)

    # Clean the data
    train_data = clean_text(train_data)
    val_data = clean_text(val_data)

    # Tokenize the data
    tokenizer = Tokenizer(num_words=10000)
    tokenizer.fit_on_texts(train_data['tweet'])
    X_train = tokenizer.texts_to_sequences(train_data['tweet'])
    X_val = tokenizer.texts_to_sequences(val_data['tweet'])

    # Pad sequences
    max_len =100
    X_train = pad_sequences(X_train, maxlen=max_len)
    X_val = pad_sequences(X_val, maxlen=max_len)

    # Convert labels to one-hot vectors
    y_train = pd.get_dummies(train_data['label']).values
    y_val = pd.get_dummies(val_data['label']).values

    # Define the model
    model = Sequential([
        Embedding(input_dim=10000, output_dim=128, input_length=max_len),
        Bidirectional(LSTM(units=64, return_sequences=True)),
        Dropout(0.2),
        Bidirectional(LSTM(units=32)),
        Dropout(0.2),
        Dense(units=2, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)

    # Save the trained model to disk
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    model_path = os.path.join(model_dir, "model.h5")
    joblib.dump(model, model_path)

    # Save the tokenizer to disk
    tokenizer_path = os.path.join(model_dir, "tokenizer.pkl")
    joblib.dump(tokenizer, tokenizer_path)

    # Evaluate the model
    y_pred_val = model.predict(X_val)
    y_pred_val = y_pred_val.argmax(axis=1)
    y_val = y_val.argmax(axis=1)
    
    # Compute performance metrics
    print('\n')
    print('Evaluation------------->')
    compute_performance(y_val, y_pred_val)

    print(f"Model saved to {model_path}")



    return

"""## Testing Method 2 Code"""

def test_method2(test_file, model_file, output_dir):
    """
      take test_file, model_file and output_dir as input.
      It loads model and test of the examples in the test_file.
      It prints different evaluation metrics, and saves the output in output directory

      ADD Other arguments, if needed

    Args:
        test_file: test file name
        model_file: model file name
        output_dir: Output Directory
        

    
    """
    print('\n')
    print('========================TESTING METHOD 2========================')
    # Load test data
    test_data = pd.read_csv(test_file)

    # Clean the test data
    test_data = clean_text(test_data)
    
    # Load the trained model and vectorizer
    model = joblib.load(os.path.join(model_file, "model.h5"))
    tokenizer = joblib.load(os.path.join(model_file, "tokenizer.pkl"))

    # Tokenize the data
    X_test = tokenizer.texts_to_sequences(test_data['tweet'])

    # Pad sequences
    max_len = 100
    X_test = pad_sequences(X_test, maxlen=max_len)


    # Make predictions on the test data
    y_pred_test = model.predict(X_test)
    y_pred_test = y_pred_test.argmax(axis=1)
    y_test = pd.get_dummies(test_data['label']).values.argmax(axis=1)

    # Compute evaluation matrics
    print('\n')
    print('Evaluation------------->')
    compute_performance(y_test, y_pred_test)


    # Save predictions to output file
    y_pred_test = np.where(y_pred_test==1, 'OFF', 'NOT') # Convert back to 'NOT' and 'OFF
    test_data['out_label'] = y_pred_test
    test_data.to_csv(output_dir, index=False)
    print(test_data)
    print("Testing completed successfully!")

    return

training_data = [
    (train_25_file, MODEL_2_25_DIRECTORY, model_2_25_output_test_file, '25%'),
    (train_50_file, MODEL_2_50_DIRECTORY, model_2_50_output_test_file, '50%'),
    (train_75_file, MODEL_2_75_DIRECTORY, model_2_75_output_test_file, '75%'),
    (train_100_file, MODEL_2_100_DIRECTORY, model_2_100_output_test_file, '100%')
]

for data in training_data:
    train_file, model_dir, output_file, data_label = data
    print(f'=====Training on {data_label} of the dataset=====')
    train_method2(train_file, val_file, model_dir)
    print('\n')
    print('='*100)
    print('\n')

"""**The best model was when trained and evaluated on 75% of training data**"""

# testing on entire dataset
test_method2(test_file, MODEL_2_100_DIRECTORY, model_2_100_output_test_file)

"""## Method 2 End

# Method 3

**This method preprocesse the data using NLTK and TFIDF Vectorizer and trains a BiLSTM model**

## Training Method 3 code
"""

def train_method3(train_file, val_file, model_dir):
    """
    Trains a Convolutional Neural Network (CNN) model on the given training dataset, validates it on the given validation
    dataset, saves the trained model, and prints the classification report.

    Args:
        train_file: str, the path of the training dataset file
        val_file: str, the path of the validation dataset file
        model_dir: str, the path of the directory where the trained model should be saved

    Returns:
        None
    """
    """
    Trains a Support Vector Machine model using the preprocessed training data and saves the
    trained model to disk.

    Args:
        train_file (str): The path to the preprocessed training data file.
        val_file (str): The path to the preprocessed validation data file.
        model_dir (str): The path to the directory where the trained model will be saved.
    """

    print('========================TRAINING METHOD 3========================')
    # Load the dataset
    train_data = pd.read_csv(train_file)
    val_data = pd.read_csv(val_file)

    # Clean the data
    train_data = clean_text(train_data)
    val_data = clean_text(val_data)

    # Vectorize the data
    X_train, y_train, vectorizer = vectorizing(train_data)
    vec_path = os.path.join(model_dir, "vec.sav")
    joblib.dump(vectorizer, vec_path)

    X_val = vectorizer.transform(val_data['tweet'])
    y_val = val_data['label']

    # Pad sequences
    max_len = 100
    X_train = X_train.toarray()
    X_train = pad_sequences(X_train, maxlen=max_len)
    X_val = X_val.toarray()
    X_val = pad_sequences(X_val, maxlen=max_len)

    # Convert labels to one-hot vectors
    y_train = pd.get_dummies(y_train).values
    y_val = pd.get_dummies(y_val).values


    # Define the BiLSTM model
    model = Sequential([
        Embedding(input_dim=10000, output_dim=128, input_length=max_len),
        Bidirectional(LSTM(units=64, return_sequences=True)),
        Dropout(0.2),
        Bidirectional(LSTM(units=32)),
        Dropout(0.2),
        Dense(units=2, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)


    # Save the trained model to disk
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    model_path = os.path.join(model_dir, "model.h5")
    joblib.dump(model, model_path)

    # Evaluate the model
    y_pred_val = model.predict(X_val)
    y_pred_val = y_pred_val.argmax(axis=1)
    y_val = y_val.argmax(axis=1)

    
    # Compute evaluation matrics
    print('\n')
    print('Evaluation------------->')
    compute_performance(y_val, y_pred_val)


    print(f"Model saved to {model_path}")

    return

"""## Testing Method 3 code"""

def test_method3(test_file, model_file, output_dir):
    """
    Loads a trained model from disk and evaluates it on the test data. Saves the
    output file to disk.

    Args:
        test_file (str): The path to the preprocessed test data file.
        model_file (str): The path to the trained model file.
        output_dir (str): The path to the directory where the classification report will be saved.
    """
    print('\n')
    print('========================TESTING METHOD 3========================')
    # Load test data
    test_data = pd.read_csv(test_file)
    
    # Load the trained model and vectorizer
    model = joblib.load(os.path.join(model_file, "model.h5"))
    vectorizer = joblib.load(os.path.join(model_file, "vec.sav"))

    # Preprocess the test data
    test_data = clean_text(test_data)

    # Vectorize the data
    X_test = vectorizer.transform(test_data['tweet'])
    X_test = X_test.toarray()
    X_test = pad_sequences(X_test, maxlen=100)
    y_test = test_data['label']

    # Convert labels to one-hot vectors
    y_test = pd.get_dummies(y_test).values

    # Make predictions on the test data
    y_pred_test = model.predict(X_test)
    y_pred_test = y_pred_test.argmax(axis=1)
    y_test = y_test.argmax(axis=1)


    # Compute evaluation matrics
    print('\n')
    print('Evaluation------------->')
    compute_performance(y_test, y_pred_test)


    # Save predictions to output file
    y_pred_test = np.where(y_pred_test==1, 'OFF', 'NOT') # Convert back to 'NOT' and 'OFF
    test_data['out_label'] = y_pred_test
    test_data.to_csv(output_dir, index=False)
    print("Testing completed successfully!")
    return

training_data = [
    (train_25_file, MODEL_3_25_DIRECTORY, model_3_25_output_test_file, '25%'),
    (train_50_file, MODEL_3_50_DIRECTORY, model_3_50_output_test_file, '50%'),
    (train_75_file, MODEL_3_75_DIRECTORY, model_3_75_output_test_file, '75%'),
    (train_100_file, MODEL_3_100_DIRECTORY, model_3_100_output_test_file, '100%')
]

for data in training_data:
    train_file, model_dir, output_file, data_label = data
    print(f'=====Training on {data_label} of the dataset=====')
    train_method3(train_file, val_file, model_dir)
    print('\n')
    print('='*100)
    print('\n')

"""**It was observed that the performance remanined cosistent through out the datasize. So, the model trained on 100% dataset was used for testing.**"""

# testing on entire dataset
test_method3(test_file, MODEL_3_100_DIRECTORY, model_3_100_output_test_file)

"""##Model 3 End"""